{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open AI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Run a demo of the environment\n",
    "obs = env.reset()\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03150874  0.20358672 -0.00907854 -0.31793274]\n",
      "1.0\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Basic Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    # accelerate left if the pole is leaning left, and vice versa\n",
    "    return 0 if angle < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000): # 1000 steps max\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "#         show_state(env, step, info)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.122, 8.542547395244583, 24.0, 72.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4\n",
    "n_outputs = 1 # probability of acclerating left\n",
    "\n",
    "learning_rate = 0.01\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu,\n",
    "                         kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden, n_outputs,\n",
    "                         kernel_initializer=initializer)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "# select action out of probabilities [left, right]\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1-outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = 1. - tf.to_float(action)\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iterations = 250\n",
    "n_max_steps = 1000\n",
    "n_games_per_update = 10 # update policy every n episodes\n",
    "save_iterations = 10\n",
    "discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards = [] # all sequences of raw rewards for each episode\n",
    "        all_gradients = [] # gradients saved at each step of each episode\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients],\n",
    "                                                    feed_dict = {X: obs.reshape(1, n_inputs)})\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "        \n",
    "        # After playing n episodes, we are ready to update our policy\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean(\n",
    "                [reward * all_gradients[game_index][step][var_index]\n",
    "                    for game_index, rewards in enumerate(all_rewards)\n",
    "                    for step, reward in enumerate(rewards)],\n",
    "                axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            saver.save(sess, \"rl_model/my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pg_policy(sess, obs):\n",
    "    return sess.run(action, feed_dict={X: obs.reshape(1, n_inputs)})[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rl_model/my_policy_net_pg.ckpt\n",
      "184.298 23.88809737086652 33.0 200.0\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"rl_model/my_policy_net_pg.ckpt\")\n",
    "    \n",
    "    totals = []\n",
    "    for episode in range(500):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for step in range(1000): # 1000 steps max\n",
    "            fitted_action = pg_policy(sess, obs)\n",
    "            obs, reward, done, info = env.step(fitted_action)\n",
    "    #         show_state(env, step, info)\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        totals.append(episode_rewards)\n",
    "    \n",
    "print(np.mean(totals), np.std(totals), np.min(totals), np.max(totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = np.array([ # shape=[s, a, s']\n",
    "    [[.7, .3, .0], [1., .0, .0], [.8, .2, .0]],\n",
    "    [[.0, 1., .0], [nan, nan, nan], [.0, .0, 1.]],\n",
    "    [[nan, nan, nan], [.8, .1, .1], [nan, nan, nan]],\n",
    "])\n",
    "R = np.array([ # shape = [s, a, s']\n",
    "    [[10., .0, .0], [.0, .0, .0], [.0, .0, .0]],\n",
    "    [[10., .0, .0], [nan, nan, nan], [.0, .0, -50.]],\n",
    "    [[nan, nan, nan], [40., .0, .0], [nan, nan, nan]],\n",
    "])\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    # zero for possible actions, -inf otherwise\n",
    "    Q[state, actions] = 0.0\n",
    "\n",
    "learning_rate = .01\n",
    "discount_rate = .95\n",
    "n_iterations = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s, a] = np.sum([\n",
    "                T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))\n",
    "                for sp in range(3) # sp is next state transitioned by action a\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.88646117 20.79149867 16.854807  ]\n",
      " [ 1.10804034        -inf  1.16703135]\n",
      " [       -inf 53.8607061         -inf]]\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(Q)\n",
    "\n",
    "# Optimal actions for each state\n",
    "# It varies when you change discount_rate\n",
    "print(np.argmax(Q, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm (Off-Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 0 # start in state 0\n",
    "\n",
    "Q = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = .0\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    a = rnd.choice(possible_actions[s]) # choose action randomly\n",
    "    sp = rnd.choice(range(3), p=T[s, a]) # pick next state using T[s, a]\n",
    "    reward = R[s, a, sp]\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    Q[s, a] = learning_rate * Q[s, a] + (1 - learning_rate) * (\n",
    "        reward + discount_rate * np.max(Q[sp])\n",
    "    )\n",
    "    s = sp # move to next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132.13378789 125.52709849 125.5270137 ]\n",
      " [ 77.93009839         -inf  85.70369039]\n",
      " [        -inf 137.70415792         -inf]]\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(Q)\n",
    "print(np.argmax(Q, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning to Play Ms. Pac-Man Using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "print(obs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    return img.reshape(88, 80, 1)\n",
    "\n",
    "img = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure preprocessing_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAH/CAYAAADHdDuDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xu8bftcP/7XO0co5zh0qM0WIUmi\nfL+iIvrppojaErHk903107fL6V7y/R4quvI9vpRuclly30jCN/1EVEqEEso1+5zt7lxwFPp8/xhj\nOXNPa6352fvMNddYcz+fj8d5nL3nHPMz3p/PGGuv8R7vz2eOaq0FAACgx2ftdwAAAMDBIYEAAAC6\nSSAAAIBuEggAAKCbBAIAAOgmgQAAALpJINZIVT24qv5g2dt2tNWq6ian8LmHVtVTlhHDKlXVi6rq\ne/eo7W+qquftRdsMqurvqurL9jsOADioJBATVVUPqKp/rKqPVdV7qupxVXX2bp9prT2itfbAnvZP\nZtvT2XZJTmvtLq21J+3RLh+R5Fdn9v9L43nwyap66DbxXbuqnlpVF1XVh6vqj2beu0pV/WFVXTKe\nQz9xssFU1e2q6iVV9aGqen9VPauqDs28//VV9RdVdXFVvXObz99wfP9jVfXmqvqGXfb1xKr6j6r6\nyLi/l1TVzea2OVRVv19VF47bvX383M1m9tfG9z5SVe+tqt+uqivPNPObSX7xZMcCABhIICaoqn4y\nya8l+ekk10hyuyQ3SPKSqvrsHT5zxuoiPDgO0rhU1W2SXKO19qqZl9+a5GeS/OkOH3tOkvdkOD+u\nk+HieMtDk3zx+N7XJ/mZqvqWmf0drqprz8VQVfWVMy9dM8nvJbnh2M6lSZ4w8/5Hk/xhhnN1O09L\n8g9JPi/JLyR59vw+5/x6a+3qSa6X5IIkj5+J7fOS/HWSz0lyhyRnJrl1kpcn+ca5ds4e2/nyJF+d\n5L/PvPf8JF8/mwgBAP0kEBNTVWcleViSH2mtvbi19onW2juT3CvDBdz9xu0eWlXPrqqnVNUlSR4w\nf7e8qu5fVe+qqg9W1f+oqndu3QGe3Xbmru33VtW/VdUHquoXZtr5qqr6m/Eu9/GqeuxOicw2/blu\nVT1/vKP81qr6/rlNrlpVz6iqS6vqtVV1q5nP/mxVXTC+95aquvP4+mdV1c9V1dvGvj2zqq4115fv\nq6p/S/LSqnpxVf3wXFyvr6rvHP/86Kp693in/jVVdYfx9W9J8uAk3z3ezX79+PrLquqBM7E8ZBzn\n91XVk6vqGj3juo27ZLgY/rTW2pNaay/KcOE+P7bflOT6SX66tXbxeK78w8wm90/yS621D7fW3pTk\n95M8YOb9786QlF5z5rXzkzx6Zv8vaq09q7V2SWvtY0kem+RrZ97/u9baZpK3bxPfTTNc4J/XWrus\ntXY0yT8mObLLGGy1e1mSZyb5ipmXfzzJJUk2Wmtva4OLWmtPaK09Zod23pfkJUluPvPax5O8Jsk3\nLYoDAPhMEojp+ZokV81wZ/nTWmsfSfKinHin9e5Jnp3k7CR/NLt9Vd08yW8nuW+SQxkqGddbsO/b\nJ/mSJHdO8j+r6kvH1z+V4eLtnAx3c++c5Ic6+/O0JMeSXDfJPZM8YisRmOnDs5JcK8lTkzyvqq5c\nVV+S5IeT3Ka1dmaSb07yzvEzP5rkHknuOLb74SS/NbffOyb50vFzT01yn603xrG5QS6/q//qDBeq\nWzE8q6qu2lp7cYYpRc9orV29tXarfKYHjP99fZIbJbl6hovsWTuN67wvT/KWHd7bzu3G7Z80JlKv\nrqo7jn28Zoaxef3M9q9P8um5/621RyZ5RZIXV9WZVfWrGcbt7rvs8+uSvLEzvi9L8vbW2mzyc0IM\nO6mqz81wzN468/I3JHlua+0/O/efqrpuhnPgVXNvvSnJdscTAFhAAjE95yT5QGvtk9u8d3x8f8vf\ntNae11r7z/GO7ax7JvmT1torW2v/keR/JmkL9v2w8U7x6zNc6N0qSVprr2mtvaq19smxGvK7GS40\nd1VV189w8fyzrbWPt9Zel+QPkmzMbPaa1tqzW2ufSPKoDMnT7TIkLVdJcvOqunJr7Z2ttbeNn/nB\nJL/QWjvWWvv3DFN17jk3XemhrbWPjuPy3CRfUVU3GN+7b5LnjJ9Na+0prbUPjv175LjfL1nUv5m2\nHtVae/uY5P18knvPxbLtuG7j7GxTadjF4Qx30f8iyRckeWSSP66qczIkMkly8cz2F2eY9jPrR5P8\nc4ak4O5JvrG19uHtdlZVt8xwHu00XWne1ef2v1MMs36qqi7KMA63z4nnyjkZpmttxfPtY1Xs0qr6\ns7l2PjC2c0GGaVbPnnv/0gzjDQCcJAnE9HwgyTk7zN0/NL6/5d27tHPd2ffH6ScfXLDv98z8+WMZ\nL0Kr6qZV9YIaFuJekuGu/DnbNbBNDB+auwP9rpxYCZmN8T8zVitaa29Ncm6G5OB9VfX08W5yMlQP\nnjtePF6U4W7yp5J8/g7tXpqh2nDv8aV7Z6ZiU1U/WVVvqmEh8EUZqjU9/dvq47vm+nfGXCzbjus2\nPpzdL67nXZbkna21x4/Tl56eod9fm+Qj4zZnzWx/VuYSlNZayzB+184w9pdst6MavmXrRUl+rLX2\nis74PjK3/21jmPObrbWzM6y5uCwnJnIfzPAzsBX788dtfzzJ/JS6c8b3PifJXyV58dz7Zya5qK8b\nAMAsCcT0/E2Sf0/ynbMvjlM67pLk/595ebeKwvEMd6i3Pn+1DAtZT8Xjkrw5yRe31s7KsC6gOj53\nYZJrVdXsRfEXZrgrvOX6MzF+1hjzhUnSWntqa+32GRKGlmFheTJcJN+ltXb2zH9Xba3Ntjs/Nk9L\ncp+q+uokV8tw1z7jeoefzbDG5JrjRefFM/1bVLW5cIxvtn+fTPLeBZ/bzhuS3PQkt982vrGKcDwn\nVjtulbnpR1X1Q0kelGGNwEUZpm9deW6bGyT58wzrKTZPIr43JrnR3PH/jBh2iP/fkvxYkkeP524y\nnPv3GM+TLmMF6olJvnqszGz50pw4vQsA6CSBmJjW2sUZFlE/pqq+ZVwPcMMM6wSOJem9gHt2krtV\n1deMC54flr6L/u2cmeHO9Edq+LrMB/V8qLX27gzfmvMrVXXVcQrM9+XE9Rr/paq+c6y4nJsheXpV\nVX1JVf0/VXWVJB/PcDf6U+NnfifJw7emJNXwVaa7zdtPkhdmuND/xQxrGrbm0Z+Z4YL//UnOqKr/\nmRPvmr83yQ13uWh9WpIfr6ovqqqr5/I1E9tNQVvkhZmbGjYe/6tm+Fk9YxzHK41vPzfJNcdF2leq\nqntmqO781fj+k5M8pKquOR63789wMb3V9kaGKVff0Fp7R5LvyXCOzFZnrpfkpUl+q7X2O/MBj4vI\nr5rkysNf66rj+ZbW2r8keV2S88bXvyPJLZMc7RmM1tpLMiRoPzC+9KgM3wq1WVU3rsGZOXGh9Xx8\nV8kwDeo9GStw42v/JcPiagDgJEkgJqi19usZ7vL/ZoYL97/NcNf9zlvz9jvaeGOSH0ny9Ax3oi9N\n8r4MF+gn66cyXFxemuGbfJ5xEp+9T4bpKBdmuOA9b7ww3PLHGb4N6MMZLvS+c1wPcZUMz0P4QIaL\nv+tkGJNk+Jag5yf5s6q6NMMC2dvuFsQ4bs/JsBD3qTNv/Z8MU3P+JcP0o4/nxKlhzxr//8Gqeu02\nTf9hhqTuL5O8Y/z8j+wWyy4xvjbJxVU125ffz5A83SfD16BelnFdQGvtQ0m+PcPxuTjJzyW5e2tt\na5rbeUneNvbr5Ul+Y1wYvuWNGdY8vG1s7xMZ1s7MHt8HZlgcfl5d/myFj8y8/3VjTC/MUH25LMns\neoR7J/mvGY7vrya5Z2vt/ScxLL+R4etnrzL263YZxviVGc7H12VIAueT2ovGON+bYeH/t4/TtZJh\nzF7WWrvwJOIAAEZ1+e9U1tl4d/yiDNOQ3rHf8bC98atZf6i1do/9jmVdVdXfJvm+1to/7XcsAHAQ\nSSDWWFXdLcO88crwDT23TXLr5qADAHCKTGFab3fPMHXowgxPJL635AEAgCtCBQIAAOimAgEAAHST\nQAAAAN22e9rxylWVeVQA+6i1dqrPiQHgNKMCAQAAdJtEBeLYj/3YfocAAAB0UIEAAAC6TaIC0ePw\n0UP7HcKBdezI8YXbGF/206Jz1Pl5xfT8GwAAvVQgAACAbhIIAACgmwQCAADoJoEAAAC6SSAAAIBu\nEggAAKCbBAIAAOgmgQAAALpJIAAAgG4SCAAAoNsZ+x3AMh07cnzhNoePHtrzNqbYzjJMrU+L2pna\n+K5jv52fe9vOlMYXALaoQAAAAN0kEAAAQDcJBAAA0E0CAQAAdKvW2n7HkAvOPXdhEBYKnjoLMZm6\nZSwMZ2c9/wZc7/zzawWhALAGVCAAAIBuEggAYE9V1Z2q6tgpfvZlVfXAZce016rqI1V1oz1q+1eq\n6ty9aJvdVdUNq6pV1cJHIVTVt1fV01cR16pJIADgJFTVO6vqsvEC8b1V9YSquvp+x8X+2S7Jaa1d\nvbX29j3Y17WT3D/J7868dq+qelNVXVpV/1xV95h5r6rql6vqgqq6eIz1y05hv9eqqmdU1QfG//6o\nqs6aef+GVfUXVfWxqnpzVX3DFe3rQddae36SW1TVLfc7lmWTQADAybtba+3qSW6d5DZJHjK/wXjh\ntrTfs8tuj8v13E2ekAckeWFr7bIkqarrJXlKkp9IclaSn07y1Kq6zrj9dyX5b0nukORaSf4myeZs\ng1V16/mdVNXNqupzZl765STXTHKjJDdO8vlJHjrz/tOS/EOSz0vyC0mePSY7V8gBOzbbeVqSH9jv\nIJbNP0QAcIpaaxckeVGSWySfvhP98Kr6qyQfS3KjqrpGVT2+qo6Pd4F/uaquNG7/gKr6q6p6zHh3\n+M1Vdeet9ndo77pV9fyq+lBVvbWqvn9m+ytV1YOr6m3j3ejXVNX1x/duVlUvGT/3lqq618znvnW8\nc33pGONPja+fU1UvqKqLxs+9YiuJGeM4WlXvr6p3VNWPzrR3tap6YlV9uKr+OUOStaOq+pqqevU4\nBq+uqq+Z2+TGVfV34/t/XFXXGj931ap6SlV9cIzx1VX1+eN7PeP+v6rqQ0l+afz8LWZiunYNlabr\nVNU1x3F4/9inF1TV4XG7h2e4OH9sDVWpx46vt6q6yUwsTx4//66qesjMOD6gql5ZVb85tv2OqrrL\nLsN1lyQvn/n74SQXtdZe1AZ/muSjGS7yk+SLkryytfb21tqnMiQbN5/p51lJ/riqvm/mtZsl+Ysk\nXzuzny9K8rzW2iWttYuTPDfJl43b3zRDMn1ea+2y1trRJP+Y5Mh2Haiqz6uqP6mqS8Zj9stV9cqZ\n91tV/feq+tck/7oV03bnb1XdpoZK4Bkznz9SVa8b//xVVfX3477eW1WPmtnu9lX11+Oxf3dVPWB8\n/duq6h/Gz7y7qh6608HY7TwbvSzJt+30+YNKAgEAp2i8OP/WDHdet2xkuON4ZpJ3JXlSkk8muUmS\nr0zyTUlmp7vcNsnbk5yT5Lwkz9m6QN6hvaclOZbkuknumeQRdXnS8RNJ7jPGdFaGO88fq6rPTfKS\nJE9Ncp1xm9+uy6eyPD7JD7bWzsyQDL10fP0nx31dO8Md5wcnaePF758keX2S6yW5c5Jzq+qbx8+d\nl+EC9sZJvjnJ9+4yhtdK8qdJ/neGu9ePSvKnVfV5M5vdf+zLdcex/N/j69+b5BpJrj9+9v9Lctn4\nXu+4XyfJLyZ5zjguW+6V5OWttfdluF56QpIbJPnCcR+PTZLW2i8keUWSHx6nLf3wNt18zBjnjZLc\ncezP/zsXy1synAO/nuTxVbXTN6N9+bjtlr9P8qYa5ttfqYbpS/+e5A3j+09PcpOqumlVXXkcsxdv\nfbi1dsk4Ng+vqu+pqhsn+fMkD2mtvWRmP7+V5K5jMnXNDMnBi8b3vizJ21trl85s//rx9e38VoYk\n5wvGeLY7P+4xjsvNdzt/W2uvTvLBJN8489n75fIqy6OTPLq1dlaG8/GZSVJVXzjG/5gM5/dXJHnd\n+JmPZjhGZ2e4+H9QzUwLm7PoPHtTkhvWzHSvdSCBAICT97yquijJKzPcDX7EzHtPbK29sbX2yQxT\nRu6S5NzW2kfHi9H/leTeM9u/L8n5rbVPtNaekeHi8Nt2aO8Lktw+yc+21j7eWntdkj/IkGQkw4XL\nQ1prbxnvRr++tfbBJHdN8s7W2hNaa59srb02ydEMCUiSfCLDhdpZrbUPj+9vvX4oyQ3G+F7Rhu9/\nv02Sa7fWfrG19h/jXP/fn+nXvZI8vLX2odbau3P5Bf92vi3Jv7bWNsfYnpbkzUnuNrPNZmvtn1pr\nH03yP5Lca7zL+4kMicNNWmufaq29prV2yViFWDTuF7bWHjPu87IMF6ezCcT3jK+ltfbB1trR1trH\nxovkh2dIBBYa4/zuJD/fWru0tfbOJI/M5ccsSd7VWvv9sULwpAxj/vk7NHl2kk9fqI+fefIY67+P\n///BcayS5HiGBOctGRKf70ry47MNttbelGG8Hj1u+2uttcfP7fe1ST47w8X6B5N8Kslvj+9dPcnF\nc9tfnCHp3W48jmSoVnystfbPY5/n/cp4/lyWxefvkzIkDVsJ6TeP45AM58hNquqc1tpHWmuvGl+/\nb5I/b609bTy3Pzj+PKW19rLW2j+21v6ztfaGDEn7ZxzvzvNs61idvU0fDywJBACcvHu01s5urd2g\ntfZDW/PRR++e+fMNklw5yfFxmsRFGRa/XmdmmwvaiQ9leleGO+3btXfdJB+au9P7rgxVgGS4E/+2\nbeK9QZLbbsUwxnHfDAlJMlzQfWuSd1XVy6vqq8fXfyPJW5P8WVW9vap+bqa968619+BcftF73bm4\n37VNTLN9mn9/tk/Zpq0rZ7hbv5nk/yR5elVdWFW/Pt5l7xn32TaToepytaq6bVXdIMMd6ecmSVV9\nTlX97jj96JIkf5nk7LmpKjs5J8OF92wf5/v3nq0/tNY+Nv5xp4X5H87MhXkNi5V/Pcmdxv3cMckf\nVNVXjJuclyHhu36SqyZ5WJKX1onrG5Ih0bh43O+/bLPfZ42vn5mhuvW2DNOhkuQj42uzzspMojPj\n2knOyInjP38s5l9bdP4+Jcndavgyg3sleUVrbesBON+X5KZJ3jxOl7rr+PpOPysZz4G/GKecXZyh\nsnXONpv2nGdbx+qi7fZ1UEkgAGC5ZpOBd2e4K3zOmHCc3Vo7q7U2O7XjenPTVb4wyYU7tHdhkmtV\n1Zlz218ws78b5zO9O8N0nLNn/rt6a+1BSdJae3Vr7e4ZLnyel3Gax3jH/CdbazfKUBH4iXG61LuT\nvGOuvTNba9867u94hgu02Rh3cmGGC7FZs33KNm19IskHxjvHD2ut3TzJ12S4U33/9I37CQ+xba39\n59jv+2SoPrxgJlH7ySRfkuS241SYrxtfr+3amvOBMd7ZPs7372S8IcMF8ZavSPKXrbW/H++YvzrJ\n3ybZ+hakWyV5Rmvt2Hj3/okZFkPProM4J8O0pScn+ZYkm1V1p7n93irJ74532j+S5HcyJJ1J8sYM\n63POnNv+jdvE//4MU34Oz7x2/W22m/852u38vSDD4vDvyFDZ+fQi8dbav7bW7pPh3P61DIu7Pzc7\n/6wkQ/Xi+Umu31q7xtjX7aaU9ZxnX5qhenLJDvs6kCQQALBHxrugf5bkkVV1VlV9VlXduKpmp0Nc\nJ8mPVtWVq+q7MlxwvHCH9t6d5K+T/EoNC4hvmeEO6x+Nm/xBhgXBX1yDW45rCV6Q5KZVtTHu58rj\n4tMvrarPrqr7VtU1WmufSHJJhukpqaq7VtVNxgRn6/VPJfm7JJdU1c/WsGD6SlV1i6raWiz9zCQ/\nP86XP5zkR3YZpheOsX1PVZ1RVd+d4eL2BTPb3K+qbj7eNf/FJM9urX2qqr6+qr58rARckuFC/VOd\n476dp2aYbnTfXD4FJhnuIl+W5KJxisx5c597b4b1DZ9hnGL0zAxrDM4cqxs/kcvv3p+sF+bE6TSv\nTnKHrYpDVX1lhkXdb5h5/7uq6vPHcdjIcNf8reP2V88wVi8Yp6T9dYYpOM+qqq+a288Dx+N9tQzr\ncl4/9vFfMqwfOG88L78jyS0zTDPabjyek+ShY2XnZhmSvt3seP7ObPPkJD+TYY3Ic7derKr7VdW1\nxwRxqwrwqQw/M99Qw1fgnlHDwu6tqs2ZGSp9Hx/H4Hu2C6rzPLtjLl8rsjYO+ldjLd2xI8cXb7Qk\nh48eWtm+pmSVY8ypc37uvdN1jE9D90/yq0n+OcOFydsz3And8rdJvjjDner3JrlnG9Yt7OQ+Ge6I\nXphhOst57fLFro9KcpUMFzXnZFhL8B2ttQ9W1TeN7z8qww3E12e4kE2Gu7aPHS/E35JxPvkY12Mz\nTDv5cJLfbq29LEmq6m4Z5vK/Y9znW3L519k+bIzxHWOcT0jyY9t1Zoztrhnm3z8uw4XtXVtrH5jZ\nbDPJE5PcLMOakweNr3/BuJ/DGabRPCOXX5gvGvftYvnbqvpohmlVsxd952dIKD4w9ueRGRb5bnl0\nkidV1YMyrNf40ZzoRzIs1n17ko9nWC/yh7vFsosnJ3ldVV2tDd949PIaviXo2TXMyX9/kke01v5s\n3P7XMiSpr0vyuRnG90hrbeti+qNJfqMNa0+2xuGlYyL3bzP7/W8Z1rIcy3A3/u8yfKXslntnOEYf\nHj93z9ba+3foww+P274nw3nztCT/dacOt9YuXXD+JkPS8Lgkz51Z/5EMFZVHjcnnu5Lcu7X28ST/\nVlXfmuQ3MyTeF2c4f1+X5IcyJAWPzXC+PTM7r2FYdJ7dJ5f/PK2NOnHa5f644NxzFwaxql+063jx\n0NOnVV7ISCAOhimdE1OKZZmm9G/A9c4/f6dvfGEP1fC1kQ9srd1+v2Ph4KiqRyR5X2vt/P2OZRmq\n6teSfEFrbcdv6+ps520ZFpD/+XIiu2LGJHujtXavhRsfMGtVgZjahTIAwLK11h683zFcEeO0pc/O\n8KyI22SYhvfAXT+0uM0jGdZNvHTRtqvSWvuTDF93vHbWKoEAAGDyzswwbem6Gb7G+JFJ/vhUG6uq\nl2VYN7MxrnVgj0kgAGCfjN+I88R9DgNWavymqJsssb07Last+vgWJgAAoJsEAgAA6GYKEwBTsv9f\nDQhwelv4rXwqEAAAQDcJBAAA0M0UJgAOlI2Njf0O4cDa3NxcuI3xZT8tOkedn1dMz78BPSQQEzel\nh+NNKRamYUrnxJRiAYB1ZgoTAADQTQIBAAB0k0AAAADdrIGYuCnN2Z5SLEzDlM6JKcUCAOtMBQIA\nAOgmgQAAALqt1RQmUxgAAGBvrVUCAQDJch6YtqyHrk2tnWWYWp+W8fCxVY7vOvbb+bm37UxpfBNT\nmAAAgJMggQAAALpJIAAAgG4SCAAAoFu11vY7hlxw7rkLg1jVNywdO3J8JftJptWnVX6D1SrHmFM3\npXNiSrEs05T+Dbje+efXCkLpsfD3wSoXCq6bqS3EhHnLWBjOznr+DUiy8PeBCgQAANDN17jOWcdn\nSUytT1OLh/03pXNiSrEAwBSpQAAAAN0kEAAAQDcJBAAA0E0CAQAAdJNAAAAA3SQQAABANwkEAADQ\nTQIBAAB0k0AAAADdJBAAAEC3M/Y7gGU6duT4wm0OHz20521MrZ0pxbLKdlYZS4917Lc+7W07qzw/\n183m5ubCbTY2Nq5wG8uyKJZ1tcox5tQ5P/feQRtjFQgAAKCbBAIAAOgmgQAAALpJIAAAgG5rtYh6\nGYsJl7UgcUrtTCmWqbWzygWo69jvKbUzpViW1Y4F0gBMkQoEAADQTQIBAAB0k0AAAADd1moNBACs\nq2U8HG9ZphQL0zClc2JKsawrFQgAAKCbBAIAAOgmgQAAALpZAwEAB8CU5mxPKRamYUrnxJRiWVcq\nEAAAQDcJBAAA0O3ATGE6duT4Uto5fPTQUtpZpCfeKcWyLKvqU7Lafh0k63gMptandfzZBYBeKhAA\nAEC3A1OBAIBeFlEC7B0VCAAAoJsEAgAA6CaBAAAAukkgAACAbhIIAACgmwQCAADoJoEAAAC6eQ4E\nAGxjHZ8lMbU+TS0e9t+UzokpxTI1Eog5x44cX7jN4aOHVhDJYGrxLMM69umgWcdjMLU+TS0eAFgW\nU5gAAIBuEggAAKCbBAIAAOhmDcScqc1Jnlo8y7COfTpo1vEYTK1PU4sHAJZFBQIAAOgmgQAAALpJ\nIAAAgG7WQACwdjY3Nxdus+ghUctoY2rtTCmWVbazylh6rGO/9Wlv21nl+dlDBQIAAOgmgQAAALpJ\nIAAAgG4SCAAAoFu11vY7hlxw7rkrC2JVD3c6duT4wm2mFMuyrPLhWavs10Gyjsdgan1ax5/d651/\nfq1sZ7tb+PtglQsFAdZJz2LsJAt/H6hAAAAA3SbxNa6rvLt4OlrX8V3Xfh0kjsHeWuX4tvNXtisA\nDjgVCAAAoJsEAgAA6DaJKUxw0LzqFi/b9f3b/dOdVhIHAMCqqUAAAADdVCCg06Kqw07bqkYAAOtE\nBQIAAOimAgELbFUTtqsk7PTebAVit88DABw0KhAAAEA3FQjotNu6BlUGAOB0oQIBAAB0U4EA4EDZ\n3NxcSjsbGxtLaWeRnninFMuyrKpPyWr7dZCs4zGYWp/W8We3x1olEMeOHF+4zeGjh/a8jWWaUp8O\nWjvLiuXYW4b/7zY9qWfq0kHr95TaWWUsyzKln10AWCZTmAAAgG5rVYGAvbTbg+RO5iFzAAAHmQoE\nAADQba0qEMuYCzy1+cRT6tM6ttPTxqtu8ZYrvJ/efa2qnSnFsqx2/OwCwGqoQAAAAN0kEAAAQDcJ\nBAAA0G2t1kAAwLJM6SFSyfTiWYZ17NNBs47HYGp9mlo8y6ACAQAAdJNAAAAA3UxhggVu9093SnLi\nw+LmX9vp7/OfAwA46CQQALCNqc1Jnlo8y7COfTpo1vEYTK1PU4tnGSQQ0Gm2qrDTaz3bAAAcZNZA\nAAAA3SZRgTh25PjK9nX46KGV7WuRVfZ7VVY5vus4fsvgGKwP4wvAFKlAAAAA3SQQAABANwkEAADQ\nTQIBAAB0k0AAAADdJBAAAEBBeiGVAAAQw0lEQVQ3CQQAANBNAgEAAHSTQAAAAN0kEAAAQLcz9juA\nqTl25PjCbQ4fPbSCSNbXqsZ4XY9Tz/gto411Hb9VMcanZmNjY79DWGvrOr772a8f+IEf2PX93/u9\n31tRJPtrXc+tqVjl+G5ubi7cRgUCAADoJoEAAAC6SSAAAIBu1kDMMSd57xnj/ecY7D1jDOtnu/UO\ni9Y4nMpnYOpUIAAAgG4SCAAAoJsEAgAA6GYNBABrp+d7zBd9r/oy2limKfXpoLWzrFhe8YpXLNym\nx0Hr95TaWWUsyzKln91lUYEAAAC6SSAAAIBuEggAAKCbBAIAAOhWrbX9jiEXnHvuyoJY1cOdjh05\nvpL9TM0qH561aIzX9UFeU+q383xvrXJ8r3f++bWyne1iY2Nj/38pLdnUFj9y6rZ7KNyp8CC5g+F0\n/dnd3Nxc+PtABQIAAOgmgQAAALpN4jkQy5oOMKXpFOs6fQZmOc/337KOQTt/Kc0AcBpQgQAAALpJ\nIAAAgG4SCAAAoJsEAgAA6CaBAAAAuk3iW5gAAKbOA+BgoAIBAAB0k0AAAADdJBAAAEA3ayAAmIzN\nzc2V7WtjY2Nl+1pklf1elVWO7zqO3zI4ButjauMrgZhz7MjxhdscPnroQLUzpViW2Q6nbmrHclE7\nU4pl1e0AwNSYwgQAAHSTQAAAAN0kEAAAQDdrIOYsa07ylNqZUizLbGc/veD+T9/1/bs++d4riuTU\nTO1YOs8B4OBQgQAAALqpQECnRVWHnbadejUCAOBkqEAAAADdVCBgga1qwnaVhJ3em61A7PZ5YLp6\nHtw0pYfRHUSrGuN1PU7LeLiY83zvreMYq0AAAADdJBAAAEA3U5ig024Lo01TAgBOFxIIANjGQZuT\nfBAZ4/3nGOy9dRxjCQR02q26oPIAAJwurIEAAAC6TaICcezI8f0OYenWsU89Dh89tN8h7JndHiR3\nMg+ZWyfO8/13uh4DAPaPCgQAANBNAgEAAHSTQAAAAN0kEAAAQDcJBAAA0E0CAQAAdJNAAAAA3SQQ\nAABAt0k8SA6m7K5PvneSEx8WN//aTn+f/xwAwEGnAgEAAHRTgZhz7MjxhdscPnpoBZGsr4M6xrNV\nhZ1e69lmCg7qMThIjPGp2djYWEo7m5ubS2lnGZbVJ5gy5/n+W+W/nyoQAABANwkEAADQTQIBAAB0\nswZijjnJe29VY9wzB/105Tzfe8YYgHWlAgEAAHSTQAAAAN0kEAAAQDdrIABgGz3fhd7zvetTamdK\nsSyzHU7d1I7lonamFMuq25kSFQgAAKCbBAIAAOgmgQAAALpJIAAAgG4WUQPANpa1qHFK7UwplmW2\ns58e9KAH7fr+4x73uBVFcmqmdiyd5weDCgQAANBNAgEAAHSbxBSmw0cP7XcI++J07feyGL/9t47H\n4NiR4/sdwqetcnzb+SvbFQAH3CQSCACAqdtuvcOiNQ6n8hmYOgkEnII3/Opfbvv6LX/u6z7j/a3X\nAADWgTUQAABANxUI6LRT1WG7bWarDtu9BgBwUKlAAAAA3VQgYAnm1z5YAwEArCsVCAAAoJsKBHTa\nrpKwXcUBAGCdqUAAAADdVCAAmIzNzc39DmHp1rFPPTY2NvY7hJXY7kFxpyPn+f5b5TFYqwTi2JHj\nC7c5fPTQnrexTFPq00FrZxWxzE9r2m0q0zr1e9XtrDKWZZnSzy4ALJMpTAAAQDcJBAAA0E0CAQAA\ndFurNRDLmAs8tfnEU+rTOrZzRds4ma9vXad+T7EdP7sAsBoqEAAAQLe1qkDAXvKwOAAAFQgAAOAk\nqEDAHph/PgRw8PQ8lGlKD5E6iA7aGD/ucY/b7xCW7qAdg4NoHcdYBQIAAOgmgQAAALqZwgSdZqcl\n7bSg2tQlAGDdSSAAYBsHbU7yQbSqMe6Zg366cp7vvXUcYwkEnAKVBgDgdGUNBAAA0G0SFYhjR46v\nbF+Hjx5a2b6momd8VzkuU4pnledejyn1e0rnxOn4c5tM7/wEgEQFAgAAOAkSCAAAoJsEAgAA6CaB\nAAAAukkgAACAbhIIAACgmwQCAADoJoEAAAC6SSAAAIBuEggAAKDbGfsdwNQcO3J84TaHjx5aQSSD\nKcUzpViSxfGsMpZVmlK/p3ROTCmWZHrxHBQbGxv7HcK+OF37vSz7OX5PecpTTvoz97vf/fYgkv21\njufw5ubmfofwaasc355+q0AAAADdJBAAAEA3CQQAANDNGggAgCWZX99wKmskYOokEHOmtqhxSvFM\nKZZkevGsypT6LZadTS0eAFgWU5gAAIBuEggAAKCbKUwArJ2e7zFf9L3qy2hjmabUp4PWzl7GcirP\ndFiHfu9XO6uMZVmm9LO7LCoQAABANwkEAADQTQIBAAB0k0AAAADdLKIGYO0sYzHhKhck9phSn9ax\nnWXF0vPguHXs95Ta8bO791QgAACAbhIIAACg24GZwnT46KH9DuGkHLR4T1eOE/OmdE4sK5ZjR44v\npR0ASA5QAgEAMDWL1jycyoPmYOpMYQIAALpJIAAAgG4SCAAAoJsEAgAA6GYRNQDAKbJImtORCgQA\nANBNAgEAAHSTQAAAAN2sgQBgMjY3N1e2r42NjZXtayp6xneV4zKleFZ57vWYUr+ndE6cjj+3yfTO\nz7VKII4dOb5wm8NHD+15G1NsZxmmFEuyOJ6pje+qzokpHYPk4J2fU2pnSuMLAFtMYQIAALpJIAAA\ngG4SCAAAoNtarYFYxlzgZc0nnlo7yzClWJJpHe9V7mtKx2EdY5lSO1MaXwDYogIBAAB0k0AAAADd\nJBAAAEC3tVoDAQDLctAesJV4+NhO1vXhY1Pq95TOiSnFkkwvnmVQgQAAALpJIAAAgG4SCAAAoJs1\nEACwjanNSZ5SPFOKJZlePKsypX6LZWdTi2cZVCAAAIBuEggAAKCbKUxzjh05vrJ9HT56aGX7mpIp\n9XuVx7vHqsZmSsdgavwbAAC7U4EAAAC6SSAAAIBuEggAAKCbBAIAAOgmgQAAALpJIAAAgG4SCAAA\noJsEAgAA6CaBAAAAukkgAACAbmfsdwDLdOzI8YXbHD56aAWRLM+U+rSsWFbVzkE71r2W0e+pHctl\nmFIsy7KOfVqGjY2N/Q7hpBy0eE9XjhPzpnROLCuWzc3NpbSjAgEAAHSTQAAAAN0kEAAAQDcJBAAA\n0G2tFlGv42LCKfVpWbFMrZ2DZhn9XsdjMKVYlmUd+wTAwacCAQAAdJNAAAAA3dZqChMAJH3fdb7o\ne9WX0cYU21mGKcWSLI5nauO7qnNiSscgOXjn55TamdL4JioQAADASZBAAAAA3SQQAABANwkEAADQ\nzSJqANbOMhYTLmtB4tTaWYYpxZJM63ivcl9TOg7rGMuU2pnS+CYqEAAAwEmQQAAAAN1MYSJJcuzI\n8f0O4QSHjx7a7xDW2tSONwBwcKhAAAAA3daqAtFzV9WdbQAAOHUqEAAAQDcJBAAA0E0CAQAAdJNA\nAAAA3SQQAABANwkEAADQTQIBAAB0W6vnQADAsmxubq5sXxsbGyvb15RMqd+rPN49VjU2UzoGU+Pf\ngJ1JICbOw/F2tmhs1nVcTtd+L+JnBQBWwxQmAACgmwQCAADoJoEAAAC6WQMxceZs7+x0HZvTtd+L\nGBcAWA0VCAAAoNtaVSDcgQQAgL2lAgEAAHRbqwoEACR9D4A6aA9umlKflhXLqto5aMe61zL6PbVj\nuQxTimVZptYnFQgAAKCbBAIAAOgmgQAAALpZAwHA2jlo85t7TKlPy4plau0cNMvo9zoegynFsixT\n65MKBAAA0E0CAQAAdDOFac46PoxuHfu0LKfr2Jyu/e5hbABgdyoQAABANwkEAADQTQIBAAB0k0AA\nAADdJBAAAEA3CQQAANBNAgEAAHSTQAAAAN0kEAAAQDcJBAAA0O2M/Q5gmY4dOb5wm8NHD+15G1Nr\nZ0qxrLKdVcbSYx37rU97284qz891s7m5uXCbjY2NFURy8PSM3So5Tntrasebg0EFAgAA6CaBAAAA\nukkgAACAbhIIAACg21otol7GYsJlLUicUjtTimVq7axyAeo69ntK7UwplmW1Y4E0AFOkAgEAAHST\nQAAAAN0kEAAAQLe1WgMBAOvKw/F2tmhs1nVcTtd+L+JnZe+pQAAAAN0kEAAAQDcJBAAA0M0aCAA4\nAMzZ3tnpOjana78XMS57TwUCAADoJoEAAAC6HZgpTMeOHN/vEAAA4LR3YBIIAOhlDjTA3jGFCQAA\n6CaBAAAAuk1iCtPhRz96v0MAOK2188/f7xAAOCBUIAAAgG6TqEDsh5e85DZJkm/8xlef8PftbG2z\nqlh2i2evY4F19OJb3/qEv3/La1+7T5EAwMGnAgEAAHQ77SoQJ1N52Okz6xgLrKOtysN8xWG+IrHd\nNgDA9lQgAACAbqddBQIAeqzjw+jWsU/LcrqOzena7x7GZmenXQKx03Shk5lGtNex7Fc8sG56piVt\nbbPTdCcA4ESmMAEAAN1OuwrETnarAuyHnSokwN5QiQCAPioQAABANxWI0dTu8E8tHlgHu1UXtvtq\nVwDgM6lAAAAA3VQggLV3KpUHayEAYHsqEAAAQDcVCADWzubm5sJtFj0kahltTK2dKcWyynZWGUuP\ndey3Pu1tO6s8P3tIIEa+xhXWn2lJAHDFmcIEAAB0U4HYxZSqAFOKBQ6KRQukk898gNz86wDAiaq1\ntt8xpKpWHsR2U5YW2auL9inFAuuk59kOEohBa632O4bRwt8Hq5znC7BOetZSJFn4++C0rUBsXYCf\nysX7sk0pFjhdnW4JAwCcKmsgAACAbqdtBWJLz93/VU0Xmt3PTvGYugT9TE8CgOVTgQAAALqd9hWI\nLVO7sz+1eOAgU3EAgOVRgQAAALpJIAAAgG4SCAAAoJsEAgAA6CaBAAAAukkgAACAbhIIAACgmwQC\nAADoVq21/Y4hVbX/QcAaesX5d9j29Tuc+4oVR8LUtdZqv2MY+X0AsL8W/j5QgQAAALqpQMCa2a7q\nsFVxmH9PJYItKhAAjFQgAACA5TljvwMAgC1VUymEJJdccslnvHbWWWftQySD+Xj2MxY46J74xCd+\nxmsPeMADVh7HFPXMTpJAwJrYberS/N+3tp39jOlMAEAPU5gAAIBuEggAAKCbBAIAAOhmDQQAZPtF\n04u22auFzKcSS2JhNfSyYPqKUYEAAAC6eZAcrBkPkuNUTOVBcvv5+6Dnrv+8/axAbEcFAriien4f\nqEAAAADdVCBgjW1XjUhUHvhMU6lAADB9KhAAAEA3CQQAANDNFCYATGECoJsKBAAA0E0CAQAAdJNA\nAAAA3SQQAABANwkEAADQTQIBAAB0k0AAAADdJBAAAEA3CQQAANBNAgEAAHSTQAAAAN0kEAAAQDcJ\nBAAA0E0CAQAAdJNAAAAA3SQQAABANwkEAADQTQIBAAB0k0AAAADdJBAAAEA3CQQAANBNAgEAAHST\nQAAAAN0kEAAAQDcJBAAA0E0CAQAAdJNAAAAA3SQQAABANwkEAADQTQIBAAB0k0AAAADdJBAAAEA3\nCQQAANBNAgEAAHSr1tp+xwAAABwQKhAAAEA3CQQAANBNAgEAAHSTQAAAAN0kEAAAQDcJBAAA0E0C\nAQAAdJNAAAAA3SQQAABANwkEAADQTQIBAAB0k0AAAADdJBAAAEA3CQQAANBNAgEAAHSTQAAAAN0k\nEAAAQDcJBAAA0E0CAQAAdJNAAAAA3SQQAABANwkEAADQTQIBAAB0+7/1oY1jNHHhLAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2390384358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (88×80 greyscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"preprocessing_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = [\"SAME\"] * 3 \n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "n_hidden_in = 64 * 11 * 10  # conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n  # 9 discrete actions are available\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state / 128.0 # scale pixel intensities to the [-1.0, 1.0] range.\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "                conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                conv_paddings, conv_activation):\n",
    "            prev_layer = tf.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size=kernel_size,\n",
    "                strides=strides, padding=padding, activation=activation,\n",
    "                kernel_initializer=initializer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = tf.layers.dense(last_conv_layer_flat, n_hidden,\n",
    "                                 activation=hidden_activation,\n",
    "                                 kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(hidden, n_outputs,\n",
    "                                  kernel_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,\n",
    "                                            input_channels])\n",
    "online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n",
    "target_q_values, target_vars = q_network(X_state, name=\"q_networks/target\")\n",
    "\n",
    "copy_ops = [target_var.assign(online_vars[var_name])\n",
    "            for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/conv2d/bias:0': <tf.Variable 'q_networks/online/conv2d/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " '/conv2d/kernel:0': <tf.Variable 'q_networks/online/conv2d/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " '/conv2d_1/bias:0': <tf.Variable 'q_networks/online/conv2d_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/conv2d_1/kernel:0': <tf.Variable 'q_networks/online/conv2d_1/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       " '/conv2d_2/bias:0': <tf.Variable 'q_networks/online/conv2d_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/conv2d_2/kernel:0': <tf.Variable 'q_networks/online/conv2d_2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " '/dense/bias:0': <tf.Variable 'q_networks/online/dense/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " '/dense/kernel:0': <tf.Variable 'q_networks/online/dense/kernel:0' shape=(7040, 512) dtype=float32_ref>,\n",
       " '/dense_1/bias:0': <tf.Variable 'q_networks/online/dense_1/bias:0' shape=(9,) dtype=float32_ref>,\n",
       " '/dense_1/kernel:0': <tf.Variable 'q_networks/online/dense_1/kernel:0' shape=(512, 9) dtype=float32_ref>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-4c18427ff6f6>:8: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.95\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1, keep_dims=True)\n",
    "    error = tf.abs(y - q_value)\n",
    "    clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "    linear_error = 2 * (error - clipped_error)\n",
    "    loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 500000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 4000000  # total number of training steps\n",
    "training_start = 10000  # start training after 10,000 game iterations\n",
    "training_interval = 4  # run a training step every 4 game iterations\n",
    "save_steps = 1000  # save the model every 1,000 training steps\n",
    "copy_steps = 10000  # copy online DQN to target DQN every 10,000 training steps\n",
    "discount_rate = 0.99\n",
    "skip_start = 90  # Skip the start of every game (it's just waiting time).\n",
    "batch_size = 50\n",
    "iteration = 0  # game iterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1830784\tTraining step 455196/4000000 (11.4)%\tLoss 3.304318\tMean Max-Q 40.837618    "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + \".index\"):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
    "            iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "        if done: # game over, start again\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # skip the start of each game\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "\n",
    "        # Let's memorize what happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        # Compute statistics for tracking progress (not shown in the book)\n",
    "        total_max_q += q_values.max()\n",
    "        game_length += 1\n",
    "        if done:\n",
    "            mean_max_q = total_max_q / game_length\n",
    "            total_max_q = 0.0\n",
    "            game_length = 0\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "        \n",
    "        # Sample memories and use the target DQN to produce the target Q-Value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size))\n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "        # Train the online DQN\n",
    "        _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "            X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # Regularly copy the online DQN to the target DQN\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "\n",
    "        # And save regularly\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "n_max_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig,\n",
    "                                   update_scene,\n",
    "                                   fargs=(frames, patch),\n",
    "                                   frames=len(frames),\n",
    "                                   repeat=repeat,\n",
    "                                   interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x7f238cdf7eb8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlJsJbF5UXq0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f238e858c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
